# -*- coding: utf-8 -*-
"""Copy of MLP_Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q1zaa1ikOva4bUDcfLQhSEA9vnkZ950V

# Machine Learning in Python - Project 2

*Fanny Coa, Matthew Cox, Anas Othman, Shahnazam Shahid*

## 0. Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# Add any additional libraries or submodules below

# Display plots inline
# %matplotlib inline

# Data libraries
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# Plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Plotting defaults
plt.rcParams['figure.figsize'] = (8,5)
plt.rcParams['figure.dpi'] = 80

# sklearn modules
import sklearn

# Commented out IPython magic to ensure Python compatibility.
# Display plots inline
# %matplotlib inline

# Data libraries
import pandas as pd
import numpy as np

# Plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Plotting defaults
plt.rcParams['figure.figsize'] = (8,5)
plt.rcParams['figure.dpi'] = 80

# sklearn modules
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error

# Other necessary packages
from sklearn.datasets import load_iris           # for the Iris data
from IPython.display import Image                # displaying .png images
from sklearn.preprocessing import StandardScaler # scaling features
from sklearn.preprocessing import LabelEncoder   # binary encoding
from sklearn.pipeline import Pipeline            # combining classifier steps
from sklearn.preprocessing import PolynomialFeatures # make PolynomialFeatures
import warnings # prevent warnings
from time import time
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold
from scipy.stats.distributions import uniform, loguniform

from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from imblearn.metrics import classification_report_imbalanced
import re
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

### For SVM ### Will join the two cells (This one and the one above) later

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC, LinearSVC           # SVM
from mpl_toolkits.mplot3d import Axes3D          # 3d plots
from sklearn.preprocessing import StandardScaler # scaling features
from sklearn.preprocessing import LabelEncoder   # binary encoding
from sklearn.pipeline import Pipeline            # combining classifier steps
from sklearn.preprocessing import PolynomialFeatures # make PolynomialFeatures
from sklearn.datasets import make_classification, make_moons  # make example data
import warnings # prevent warnings
import joblib # saving models
from time import time
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold
from scipy.stats.distributions import uniform, loguniform
import itertools
from sklearn.model_selection import GridSearchCV, KFold
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from imblearn.metrics import classification_report_imbalanced
import re
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# tidy the output into a dataframe
def tidy_scores(score_dict):
    df = pd.DataFrame(score_dict)
    df.loc['mean'] = df.mean()
    df.loc['sd'] = df.std()
    df.rename({"test_score":"val_score"}, axis=1, inplace=True)
    df.index.name = "fold"
    return df.round(2)


# this creates the matplotlib graph to make the confmat look nicer
# IT WORKS FOR ONLY BINARY CLASSIFICATION CASE !
def pretty_confusion_matrix(confmat, labels, title, labeling=False, highlight_indexes=[]):

    labels_list = [["TN", "FP"], ["FN", "TP"]]

    fig, ax = plt.subplots(figsize=(4, 4))
    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)
    for i in range(confmat.shape[0]):
        for j in range(confmat.shape[1]):
            if labeling:
                label = str(confmat[i, j])+" ("+labels_list[i][j]+")"
            else:
                label = confmat[i, j]


            if [i,j] in highlight_indexes:
                ax.text(x=j, y=i, s=label, va='center', ha='center',
                        weight = "bold", fontsize=18, color='#32618b')
            else:
                ax.text(x=j, y=i, s=label, va='center', ha='center')

    # change the labels
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        ax.set_xticklabels(['']+[labels[0], labels[1]])
        ax.set_yticklabels(['']+[labels[0], labels[1]])

    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    ax.xaxis.set_label_position('top')
    plt.suptitle(title)
    plt.tight_layout()

    plt.show()

# For ROC curve plotting function

def roc_plot(threshold=0.5):
    i = (np.abs(roc.threshold - threshold)).idxmin()

    sns.lineplot(x='false positive rate', y='true positive rate', data=roc, ci=None)

    plt.plot([0,1],[0,1], 'k--', alpha=0.5) # 0-1 line
    plt.plot(roc.iloc[i,0], roc.iloc[i,1], 'r.')

    plt.title("threshold = %.2f" % threshold)
    plt.show()

# For iris data example

def species_label(theta):
	if theta==0:
		return raw_data.target_names[0]
	if theta==1:
		return raw_data.target_names[1]
	if theta==2:
		return raw_data.target_names[2]

# Load data
import os
from google.colab import drive

drive.mount('/content/drive')
os.chdir('drive/My Drive/Colab Notebooks/mlp/project-2')
!ls

d = pd.read_csv("hotel.csv", index_col = False)

country = d['country']
country

"""## 1. Introduction

Booking cancellations have a significant impact on demand-management decisions in the hospitality industry. We have been hired to help provide a better understanding on why customers cancel their reservations. The two hotels present in the data we are going to be observing are individual representative examples of our employers resort and city properties. Also in our data is 30 other features to give us more information. These include deposit type, lead time, country etc.

We will be constructing a predictive model that can accurately predict when a booking will be canceled with a deeper look at the accuracy. To do so we develop an understandable validated model for the `is_canceled` variable as the outcome of interest using features derived from the data.  

We ultimately decide on using a logistic regression model as our predictive model. We altered the model output to reflect the hotel owner's approach to running the hotel, leaning more towards False Positives than False Negatives, this mimicked the common hotel practice of over-booking hotel rooms.

Using this model we have found that the country of origin of the guest, lead time, number of previous cancellations, repeated guest, number of special requests, required parking spaces, and the number of babies all had a significant effect on the likelihood of cancellation.

We discuss further the exact effect these features had on the cancellation proability, some were negative, some were positive. And highlighted which guest characteristics the hotel owner should look out for.

## 2. Exploratory Data Analysis and Feature Engineering

### Overview

In this part, we would like to visualize some features and show statistical relationship with target variable, `is_canceled`. This analysis will help to get overall view and deep familiarity of the data, detect extreme values and identify obvious errors.

We will also create new variables such as `prop_babies`, which is the proportion of babies for each reservations. These variables will be added to the data frame `d`.
"""

#create new column for proportion of babies
d['prop_babies'] = d['babies']/(d['babies']+d['children']+d['adults'])

"""There are 119,390 reservations, where for each reservation is accompanied by informations, such as whether the reservation is canceled or not, which hotel is the reservation for, the arrival date, etc. There are 31 such information in the form of columns. Of these 31 coulmns we can see that there is 16 integer valued columns, 10 categorical columns and 5 colums of the float data type."""

d.shape

#d.head()

#d.dtypes

#d.dtypes.value_counts()

"""It is also important to check for duplicates.  Here, we have a combination of features that make it unlikely to be repeated in unison, then for any bookings with the same combination of features will be considered duplicates. As shown by the result below, the data set does have duplictaes (32,252), which we will drop."""

#Check if data holds duplicate values.
d.duplicated().any()

d.duplicated().sum()

#Drop Duplicate entries
d=d.drop_duplicates(inplace= False)# will keep first row and others consider as duplicate

#d.describe().T

#d.describe(include = "object").T

"""### EDA

The plot below explores the `hotel` feature, which denotes the type of hotel the reservation is for. From the plot below, we can see the proportion of reservations made and the proportion of cancellations, for each hotel.
"""

# Hotel types details
sns.color_palette("Set2", 8)
sns.set_palette('Set2')
sns.countplot(x='hotel', data = d, hue = 'is_canceled', palette='Set2')
plt.title('Hotel Types', weight='bold')
plt.xlabel('Hotel', fontsize=12)
plt.ylabel('Count', fontsize=12)

"""#### Get Cancellation Rate

Let's look into the target value of data. The code below gives us the numbers that cannot be read off the previous plot. Approximately 38.9% of the reservations was for resort hotel and the rest of was for City Hotel. Out of all the reservations, there are 27.3% cancellations, for which 18.2% was for the City Hotel. These numbers also show that there is no balanced problem on the target value. These results could occur because the people that are likely to cancel booking do not prefer Resort Hotel and would rather go to City Hotel. This could also be due to strict cancellation policy in Resort Hotel.
"""

#d.groupby(['hotel'])['is_canceled'].value_counts()

"""#### Market Segment

The are two plots below. The first one shows the proportions of different market segment designation. 59.1% of the reservations are made through *online* travel agent, 15.9% are made through *offline* travel agent (Tour operators) and 13.5% are made directly to the hotel of the customer's choice. The second plot shows the proportion of cancellations for each market segment. 35.2% of the reservations made through the online travel agents are canceled, 25.8% of the reservations under the category `Groups` are canceled. The reservations made under the category `Corporate` has the lowest cancellation percentage of 11.9%.
"""

_, ax = plt.subplots( nrows = 2, ncols = 1, figsize = (10,8))
sns.countplot(x = 'market_segment', data = d, ax = ax[0])
sns.countplot(x = 'market_segment', data = d, hue = 'is_canceled', ax = ax[1])
plt.show()

#d.groupby(['market_segment'])['is_canceled'].value_counts()

"""#### Customer Type
Similarly, we will do the same for the variable `customer_type`, which denotes the type of booking. 82.5% of the bookings are Transient and 13.3% are Transient-Party. 30% of the Transient bookings, 14.3% of the Transient-party bookings and 16.3% of the contract bookings are canceled.
"""

_, ax = plt.subplots( nrows = 1, ncols = 2, figsize = (11,5))
sns.countplot(x = 'customer_type', data = d, ax = ax[0])
sns.countplot(x = 'customer_type', data = d, hue = 'is_canceled', ax = ax[1])
plt.show()

#d.groupby(['customer_type'])['is_canceled'].value_counts()

"""#### Cancellation in Different Years and Months

48.5% of the bookings are for the year 2016, 36.3% are for the year 2017 and the rest are for the year 2015. The cancellation rates for the year 2016 and 2017 are similar (26.2%), whereas for the year 2015, it was 20.0%.
"""

sns.countplot(x='arrival_date_year', data = d, hue='is_canceled')
plt.title('Cancellation per year', weight='bold')
plt.xlabel('Year', fontsize=12)
plt.ylabel('Count', fontsize=12)

#d.groupby(['arrival_date_year'])['is_canceled'].value_counts()

"""The graph below shows the number of bookings for each month. According to that, August is the busiest month and January is the least busy. It is half as busy as August. We see that even some months that have less booking might have more cancellations than months that have more booking. For example, we can see that December has more cancellations than November, even though November has more bookings. This tells us that the monthly booking is more interesting than the yearly as there is more seperation in the months."""

d['arrival_date_month'].replace({'January' : '1','February' : '2',
        'March' : '3','April' : '4','May' : '5','June' : '6',
        'July' : '7','August' : '8','September' : '9',
        'October' : '10','November' : '11','December' : '12'},
        inplace=True)

sns.countplot(x='arrival_date_month', data = d, hue = 'is_canceled',
              order=["1","2","3","4","5","6","7","8","9","10","11","12"])
plt.title('Arrival Month', weight='bold')
plt.xlabel('Month', fontsize=12)
plt.ylabel('Count', fontsize=12)

#d.groupby(['arrival_date_month'])['is_canceled'].value_counts()

"""#### Deposit Type

There are three types of `deposit`: No Deposit, Refundable, or Non Refund. 98.7% of the bookings are made with no deposit, 1.2% of the bookings are made with non refundable deposit, while the rest are refundable deposit. This could be due to refundable deposits not being readily offered by the hotels. Note that as the amount of people getting refunable deposits are so low that the amount of people who do/do not cancel does not show up in the second graph. 94.6% of the non refundable bookings are canceled. That might prove important feature based on how many such bookings are part of cancelled bookings.
"""

_, ax = plt.subplots( nrows = 2, ncols = 1, figsize = (9,6))
sns.countplot(x = 'deposit_type', data = d, hue = 'hotel', ax = ax[0])
sns.countplot(x = 'deposit_type', data = d, hue = 'is_canceled', ax = ax[1])
plt.show()

#d.groupby(['deposit_type'])['is_canceled'].value_counts()

"""#### Countries with High Cancellation Rate

The plot below shows the proportion of bookings made from 177 countries, with Portugal having the highest proportion.
"""

country_wise_guests = d[(d['is_canceled'] == 0)]['country'].value_counts().reset_index()
country_wise_guests.columns = ['country', 'No of guests']

country_wise_guests = country_wise_guests[country_wise_guests['No of guests'] > 60]

sns.barplot(data=country_wise_guests, x = 'country', y = 'No of guests')
plt.xticks(rotation=90,fontsize=11);

hcr_countries=d.groupby(['country'])['is_canceled'].agg(np.mean).sort_values(ascending = False)
#hcr_countries[hcr_countries>0.6]

"""There are 26 countries out of 177 countries with mean cancellation rate > 60%. There are several countries with means equal 1, which basically means that all the bookings made from the country of origin have all been canceled. Also note that some of the countries with mean equal to 1 has only one booking (such as Cambodia ('KHM')).

#### Correlation Heat Map of features
"""

mat = ['is_canceled','lead_time','stays_in_weekend_nights',
       'stays_in_week_nights','adults','children','babies','is_repeated_guest',
       'previous_cancellations','previous_bookings_not_canceled',
       'booking_changes','days_in_waiting_list','required_car_parking_spaces',
       'total_of_special_requests']

sns.heatmap(d[mat].corr(), annot=True, fmt='.2f', linewidths=2)
plt.title("Correlation Heatmap")
plt.show()

#correlation of features with the target variable, in descending order
cancel_correlation_array = d.corr()['is_canceled']
#cancel_correlation_array.abs().sort_values(ascending = False)[1:]

"""We can use the correlation plot to check what is the correlation between variables. We observe that `lead_time` is one of the highest correlated along with `adr` closely behind. We do note that there is some missing values still present so these value could become more correlated after imputation.

#### Box Plot of Numerical Features vs Cancellation Status
"""

_, ax = plt.subplots( nrows = 2, ncols = 2, figsize = (12,6))
sns.boxplot(x = 'is_canceled', y = 'lead_time', ax=ax[0,0], data = d, showfliers = False)
sns.boxplot(x = 'is_canceled', y = 'stays_in_weekend_nights', ax=ax[1,0], data=d, showfliers = False)
sns.boxplot(x = 'is_canceled', y = 'stays_in_week_nights', ax=ax[0,1], data=d, showfliers = False)
sns.boxplot(x = 'is_canceled', y = 'total_of_special_requests', ax=ax[1,1], data=d, showfliers = False)
plt.show()

"""As expected, few of the features contribute very little to tell whether the customer will cancel the booking. For example these include `stays_in_weekend_nights`, `stay_in_week_ nights`, `repeated_guest`, `previous_booking_canceled_or_not`, `required_car_parking_spaces`. Some of these have been included in the box plots above where we see that the median line is the same whether there is a cancellation or not.

We can see that the median line is not there for `total_of_special_requests`. This is because the special requests for most people are 0.

When we look at `lead_time` we can see that it is significantly different for both groups. From the box plot we can see that people with longer lead times are more likely to cancel their booking.

#### Cancellation due to the Difference in Wanted and Assigned Room Type
"""

sns.countplot(x = 'reserved_room_type', hue = 'hotel', data = d)
plt.show()

"""This figure illustrates that room types are common across the hotels. Both have room types with same names."""

_, ax = plt.subplots( nrows = 1, ncols = 2, figsize = (12,5))
sns.countplot(x = 'reserved_room_type', hue = 'is_canceled', data = d,  ax = ax[0])
sns.countplot(x = 'assigned_room_type', hue = 'is_canceled', data = d,  ax = ax[1])
plt.show()

"""Room types A,D, and E have quite high reservations and assignment status. But as a result, they have high cancellation rate as well. One possible cancellation reason could be that the allotment of unwanted room type. Let's check it. Here, we will create a new variable `unwanted_room`, which will take either *Wanted* or *Unwanted*."""

unwanted_room = np.where(d['reserved_room_type'] == d['assigned_room_type'], 'Wanted', 'Unwanted')
d['unwanted_room'] = unwanted_room

sns.countplot(x = 'unwanted_room', data = d, hue = 'is_canceled')
plt.title("No. of guests who cancelled the booking on getting room type as wanted or unwanted")
plt.show()

#d.groupby(['unwanted_room'])['is_canceled'].value_counts()

"""However, the plot above shows that a lot of customers get assigned to their desired room, yet a lot of them still canceled. The cancellation rate (45.5%) is higher compared to when the customer gets assigned to an unwanted room (4.67%).

### Feature Engineering

Having looked at the EDA, we can now look at the feautures in more detail. Here we will be handling the missing values, handling with the non-numeric variables (handling categorical, ordinal variables and strings)

#### Data Pre-Processing before applying Model

#### Missing Value Analysis
"""

Y = d[['is_canceled']]
X = d.drop(['is_canceled', 'unwanted_room'], axis=1)

num = X.select_dtypes(include='number')
char = X.select_dtypes(include='object')

# check for how many unique values each column has
def unique_vals(x):
    x = x.value_counts().count()
    return x

d_value_counter = pd.DataFrame(num.apply(lambda X: unique_vals(X)))

d_value_counter.columns = ['feature_levels']
#d_value_counter.sort_values(by = 'feature_levels', ascending=False)

"""We are interested in the amount of unique values each feature has. The feature `adr` has the most unique `feature_levels` at 8879, whereas the feature `is_repeated_guest` has the least unique value at 2. This is because there is only two values it can take. Now, we want to look at which features have 20 or more feature levels and which have less than 20. There are 10 feature that have 20 or more `feature_levels` and 10 that have less."""

slice_1 = d_value_counter.loc[d_value_counter['feature_levels']<=20]
cat_list = slice_1.index
cat = num.loc[:, cat_list]
#cat.dtypes

slice_2 = d_value_counter.loc[d_value_counter['feature_levels']>20]
num_list = slice_2.index
num = num.loc[:,num_list]
#num.dtypes
#print(slice_2)

"""Here we combine the characteristic features with the fetures that have 20 or less unique feature_levels. We leave the ones with more than this in the `num` variable."""

char = pd.concat([char, cat], axis = 1, join = 'inner')
#char.dtypes

"""Here we go on to see how many missing values there are in dataset and see that `company` has a very large amount of missing data at 94% missing. `Agent` has 14% missing data, `country` has 0.5% missing data and children (and `prop_babies` as a byproduct) has a very small amount of missing data."""

#num.isnull().mean()

#char.isnull().mean()

"""#### Outliers Analysis of Numerical Features

Here we go through our data to look for any outlier in the data as this can cause errors and give us less accurate data. By looking through the numerical data we do see that there is some outliers. We decide to make the min and max equal to the 1% and 99% percentiles respectively. This means that any data that was outside this range will be changed to be equal to the 1% and 99%.
"""

#num.describe(percentiles=[0.01,0.05,0.10,0.25,0.50,0.75,0.85,0.88,0.90,0.99])

def outlier_clip(x):
    x = x.clip(lower = x.quantile(0.01))
    x = x.clip(upper = x.quantile(0.99))
    return (x)

num = num.apply(lambda x: outlier_clip(x))
#num.describe(percentiles=[0.01,0.05,0.10,0.25,0.50,0.75,0.85,0.88,0.90,0.99])

"""By checking the numerical features we can see that some of the features have a very large variance and therfore we go on to scaling these."""

#num.var()

num['lead_time'] = np.log(num['lead_time'] + 1)
num['arrival_date_week_number'] = np.log(num['arrival_date_week_number'] + 1)
num['arrival_date_day_of_month'] = np.log(num['arrival_date_day_of_month'] + 1)
num['agent'] = np.log(num['agent'] + 1)
num['company'] = np.log(num['company'] + 1)
num['adr'] = np.log(num['adr'] + 1)

#Scaling the Variance so the solver converges

"""Here we join the nuerical and the characteristic variables together again in the X variable"""

X = pd.concat([num,char],axis=1)
#X.head()

"""### Imputation of Missing Values

Deciding an *imputation* technique is important to the role those variables will have in modeling and analysis and even how they interact with each other. Common and easy methods include *mean imputation* for *numerical variables* and *mode imputation* (most frequent) for *categorical variables*. However, these methods decrease variance, decrease the power of the variables, create misleading correlations or change correlations between variables, and importantly can often inaccurately represent the missing value. For this data, where there are a combination of numeric and non-numeric variables it was thought (especially with the large dataset) that a nearness approach would be preferable for both types. K nearest neighbour was selected as it is efficient in recognising clusters and close associations between observations. From a numeric viewpoint, this is easy to tackle, it is simply replacing the missing values with those imputed. Even if they must be kept on an integer scale, rounding is appropriate for the KNN weighted values. However, this task becomes more challenging when viewing it from a non-numeric viewpoint. The variables themselves have to be transformed into integers by an ordinal encoder and can't be maintained in this format as they would lose all meaning and interpretation. Even if there was an ordering to the integers they would have be explained thoroughly and for a feature such as `country` where there are many unique labels, this is time consuming and not an ideal way to present results. Therefore, after transforming to integers and imputing using the KNN approach a mapping procedure was created by defining the `replace_missing_mapped` function. This function found the integers that were imputed, searched for an exact matches elsewhere in the data, and when it found a match that corresponded to a non-missing value in the original data, replaced the imputed value with that label. For k=1, it finds the closest observation, however if we want to use more k's to increase the chance of a correct imputation then rounding and manipulating (to assure all are in the same format) would be needed. There is no indication as to what type of missingness is being 'applied' to these variables but this method of replacing missing values is robust and should ideally add to the model for all columns instead of just conforming to the complete data.

There are five steps that we use to do this imputation.
1. Transform categorical to factor (using encoder)
2. Remove variables with large amounts of missingness
3. Use 1 KNN so that factor variables don't need rounding
4. Impute using KNN
5. Transform categorical factors back to categorical using mapping and replace other missing values

*   Steps 1 and 2
"""

# 1. Transform categorical to factor (see references)
# 2. remove output feature and variables with large missingness
X = d.drop(['is_canceled','unwanted_room','company'], axis=1).copy()
X_nan = X.copy()
cat_cols = ['hotel','arrival_date_month','deposit_type','customer_type','reserved_room_type',
            'assigned_room_type', 'meal', 'country','market_segment', 'distribution_channel']
def encode(data):
    '''function to encode non-null data and replace it in the original data'''
    encoder = OrdinalEncoder()
    #retains only non-null values
    nonulls = np.array(data.dropna())
    #reshapes the data for encoding
    impute_reshape = nonulls.reshape(-1,1)
    #encode date
    impute_ordinal = encoder.fit_transform(impute_reshape)
    #Assign back encoded values to non-null values
    data.loc[data.notnull()] = np.squeeze(impute_ordinal)
    return data

#create a for loop to iterate through each column in the data
for columns in cat_cols:
    encode(X_nan[columns])

# check encoded dataframe has same number of missing values
#X_nan.isnull().sum()

"""*   Steps 3 and 4



"""

# 3. use 1 KNN so that factor variables don't need rounding
knn = KNNImputer(n_neighbors=1)
# 4. impute using KNN
X_imputed = pd.DataFrame(knn.fit_transform(X_nan),columns = X_nan.columns)

#check there are no missing values after imputation
#X_imputed.isna().sum()

"""*   Replace original with imputed using step 5.
For categorical variables, we must map the imputed values back to the original values and then replace missing (i.e. imputed: 135 = original: PRT). For non-categorical replace original with imputed with no need for mapping.
"""

# 5. transform categorical factors back to categorical using mapping and replace other missing values

#reset the index for allignment
X=X.reset_index()
#for loop of replacement using index of missing values from original
def replace_missing_mapped(original_data, imputed_data, column_, reference):
  column_with_na = original_data[column_]
  column_with_imputed = imputed_data[column_]
  reference_col = reference[column_]
  missing_index = np.where(X['country'].isnull())[0]
  for i in range(len(missing_index)):
    factor_index = column_with_imputed[missing_index[i]]
    where=np.where(reference_col==factor_index)[0]
    change=np.where(column_with_imputed==factor_index)[0]
    h=0
    fill = column_with_na.iloc[where[h]]
    while pd.isnull(fill):
       fill=column_with_na.iloc[where[h+1]]
       h+=1
    column_with_na.iloc[change]=fill

#replace missing values for country
replace_missing_mapped(X,X_imputed,'country', X_nan)
#replace missing values for non-categorical
X['prop_babies']=X_imputed['prop_babies']
X['agent'] = X_imputed['agent']
X['children'] = X_imputed['children']

# check that there are now no missing values in original dataframe
#X.isnull().sum()

"""## 3. Model Fitting and Tuning

### Dropping Data and Creating Dummy Variables
"""

X.drop(['arrival_date_week_number', 'stays_in_weekend_nights', 'arrival_date_month', 'agent',
                'days_in_waiting_list', 'index'], axis = 1, inplace = True)
X.shape

#Copying dataframe so that we don't need to dropcolumns from the main dataframe inplace
df = X.copy()

#one hot encoding
df = pd.get_dummies(data = df, columns = ['meal', 'market_segment', 'distribution_channel',
                                            'reserved_room_type', 'assigned_room_type', 'customer_type', 'deposit_type','country'], drop_first = True)
hotel = {'Resort Hotel': 0, 'City Hotel' : 1}
df['hotel'] = df['hotel'].map(hotel)
#df.head()

#LabelEncoder
#LE = LabelEncoder()
#df['country'] = LE.fit_transform(df['country'])
X = df

"""### Model Tuning

The model we finally agreed upon was a Logistic regression model.

We first split the data into training and test sets with a test size of 10%. We also stratified the test set according to our response variable.Due to the imbalance present in the response variable we wanted this imbalance to be proportional in our test set.

We also used LabelEncoder to transform our response variable to true ones and zeroes, as the default data had them as numeric floats.

### Logistic Regression Model
"""

## Data Split

X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify = Y,
                                                    test_size = 0.1, random_state=42)
from sklearn.preprocessing import LabelEncoder

LE = LabelEncoder()
y_train = LE.fit_transform(y_train)

"""We then fit a simple Logistic regression model as a baseline, and assessed its score. We received a convergence warning which advised us to increase the number of iterations in the model, as the solver had not converged."""

## Fitting simple Logistic Model

from sklearn.linear_model import LogisticRegression
logfit = LogisticRegression(random_state=42, penalty="none").fit(X_train, y_train)

print(logfit.score(X_test,y_test))

"""We increased the number of max iterations and fitted a simple logistic regression model again. This time we still recieve a convergence warning but our fit score did increase slightly, which means the solver again did not converge but it did get further than last time. 1000 iterations is more than enough for a model to converge assuming the dataset is not too large can be fitted to a logistic regression model. Increasing the number of iterations isn't likely to stop the warning from occuring."""

from sklearn.linear_model import LogisticRegression
logfit = LogisticRegression(random_state=42, penalty="none",max_iter=1000).fit(X_train, y_train)

print(logfit.score(X_test,y_test))

"""We then checked some metrics of our model. We seen that we had a good starting point for our model, with a AUC of 0.67 and MSE of 0.217.

"""

### Checking Scores of Simple Logreg

from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score

print("MSE of the fitted model:", mean_squared_error(y_test, logfit.predict(X_test)))
print("Accuracy of the fitted model:", accuracy_score(y_test, logfit.predict(X_test)))
print("AUC of the fitted model:", roc_auc_score(y_test, logfit.predict(X_test)))

"""Maybe we just got lucky the first time round with our splitting of our training and test sets. We decided to run K-Fold Cross Validation to see the model's performance on different partitions of the training and testing data."""

### K- Fold CV
from sklearn.model_selection import cross_validate

# One can add more model in the dictionary
model_dict = {"log": LogisticRegression(random_state=42, penalty="none",max_iter=1000)}

#print(model_dict[model_name])

for model_name in model_dict:
    linear_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("model", model_dict[model_name])])

scores = tidy_scores(cross_validate(linear_pipe, X_train, y_train, cv=10, return_train_score=True))

# For all CV results
print(scores)

#Just the mean and sd values overall
print(scores.loc[['mean', 'sd']])

"""We ran a 10-fold Cross Validation and saw that the model was very consistent when dealing with different sets of data.

With the model's consistency confirmed, we decided to interpret it's performance metrics. Since we are dealing with a binary classification problem, our model makes two types of errors.



*   *False Positive*: It assumes someone will cancel their reservation but they infact do not.
*  *False Negative*: It assumes someone will not cancel their reservation, but they do.

We can asses the model's performance via a *confusion matrix*, wich will show the number of false positives (FP),false negatives (FN), true positives (TP) and true negatives (TN).
"""

### Confusion Matrix For the Model
### This is probably how we'll discuss the results in the conclusion

from sklearn.metrics import confusion_matrix

log_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("model", LogisticRegression(random_state=42, penalty = "none",solver='lbfgs'))])

log_pipe.fit(X_train, y_train)

predictions = log_pipe.predict(X_test)


confmat = confusion_matrix(y_true = y_test, y_pred=predictions)

pretty_confusion_matrix(confmat, LE.classes_, "",  labeling=True)
coeffs = log_pipe.named_steps['model'].coef_

"""We see from the above confusion matrix, that the of the model's errors, most of them are false negatives. That is the model when incorrect, often assumes people will not cancel but they end up cancelling.

From a hotel owner's perspective this is not good, the cancellation time can be any time before the guests were due to arrive, which could result in empty rooms and thus profit loss for owner. We therefore would want a model that has it's errors in the False Positive Category instead of the False Negatives. From reseach we see that about 18.75% Are False negatives, that is we assumed they cancel, but in fact did not. We see that around 18.75% are False Negatives, that is we assumed they cancel, but in fact did not. We want False-negatives to be between 10 and 15% as most hotels overbook rooms by this much [5].

Before we started to tune our model and adjust the reward function, we wanted to see some visuals of the model performance. We plotted an ROC curve of the model. A random classifier would mean the turquoise line lined up with the dashed line, which is a good sign for us, meaning our model has predictive capabilities that are better than random classification. The closer the red dot is to the top left of the graph the better the performance of the model. Although ROC curves are good for interpreting performance, they are not ideal when there is a class imbalance, which is what we have. For moderate to large class imbalances it is more beneficial to look at the Precision-Recall curve.
"""

### ROC curve showing performance of Model

from sklearn.metrics import roc_curve, precision_recall_curve

roc_calc = roc_curve(y_true = y_test, y_score = predictions)
roc = pd.DataFrame(
    data = np.c_[roc_calc],
    columns = ('false positive rate', 'true positive rate', 'threshold')
)

# Values for ROC based on different threshold
print(roc)

#Print the related AUC value
print("AUC value:", roc_auc_score(y_true = y_test, y_score = predictions))

#ROC curve drawing by using the helper function roc_plot above

roc_plot(threshold=0.5)

"""We therfore plotted a Precision-Recall curve. When interpreting Precision-Recall curves, we are looking for a large area under the curve, which indicates that the model is returning accurate results and the majority of them being true positive results.


"""

from sklearn.metrics import PrecisionRecallDisplay

display = PrecisionRecallDisplay.from_estimator(log_pipe, X_test, y_test, name="LogisticRegression")
_ = display.ax_.set_title("2-class Precision-Recall curve")

"""By now we had decided it was time to tune our model in an attempt to improve it's performance. We attempted a Grid Search and Randomized Search in order to find the best hyper-parameters for performance. Before we began searching for hyper-parameters, we needed to ensure each fold had the same distribution of the classes. If we did not do this, we might end up with a validation score that is not a true representative of the model's performance.

We used a Stratified K-Fold to ensure the distribution of classes in our folds reflects the distribution in the over-all data.
"""

### Using Stratified K-Fold for hyper-parmater optimization

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold

KF = KFold(n_splits=5)
SKF = StratifiedKFold(n_splits=5)

fold_names = ["KFold", "StratifiedKFold"]
for i, K in enumerate([KF, SKF]):
    for j, (train_i, test_i) in enumerate(K.split(X_train, y_train)):
        fold_no = pd.DataFrame(pd.Series(y_train[test_i]).value_counts(), columns=["Fold "+str(j)])
        if j == 0:
            fold_nos = fold_no
        else:
            fold_nos = pd.concat([fold_nos,fold_no], axis=1)

    print(fold_nos)

"""Now that we've got our folds, we conducted a Grid-Search to find an Optimal C for our logistic regression. This C represents regularization strength, smaller C values implies more regularization."""

### Grid-Search to find optimal C for logisitc regrssion

from scipy.stats.distributions import uniform, loguniform
c=10.0
log_pipe = Pipeline([
            ("scaler", StandardScaler()),
            ("model", LogisticRegression(C = c, random_state=42,max_iter=1000))])

C_list = []
pwr = -5
for i in range(4):
    C_list.append(2**pwr)
    pwr+=2

# specify parameters and distributions to sample from
log_param_dist = {'model__C':loguniform(C_list[0], C_list[-1])}

log_rs = RandomizedSearchCV(log_pipe,
                            param_distributions = log_param_dist,
                            n_iter = 10,
                            scoring = ["accuracy", "f1","recall","precision"],
                            cv = StratifiedKFold(n_splits=5),
                            refit = "accuracy",
                            random_state = 42,
                            return_train_score = True,)

log_rs.fit(X_train, y_train)

"""After completing the Grid-Search, we wanted to find out which C value resulted in the best model. And we Found that a C value of 0.656089 resulted in the highest accuracy."""

### Top models found via gridsearch

log_rs_df = pd.DataFrame(log_rs.cv_results_)
log_rs_df.sort_values("mean_test_accuracy", ascending=False)[["param_model__C",
                                                           "mean_train_accuracy",
                                                           "std_train_accuracy",
                                                           "mean_test_accuracy",
                                                           "std_test_accuracy"]].head()

"""We then tried to imrpove the model by addressing it's imbalance issue.

We did this by weighting the classes during training. During model fitting we can assign a larger penalty to wrong predictions on the minority class which in our case is cancellations.


"""

log_pipe = Pipeline([
            ("scaler", StandardScaler()),
            ("model", LogisticRegression(C = c, random_state=42,max_iter=1000))])

# specify parameters and distributions to sample from
log_param_dist = {
    'model__C':loguniform(C_list[0], C_list[-1]),
    'model__class_weight': [None, "balanced"]
}

bal_log_rs = RandomizedSearchCV(log_pipe,
                            param_distributions = log_param_dist,
                            n_iter = 10,
                            scoring = ["accuracy", "f1","recall","precision"],
                            cv = StratifiedKFold(n_splits=5),
                            refit = "recall",
                            random_state = 42,
                            return_train_score = True)

bal_log_rs.fit(X_train, y_train)

bal_log_rs_df = pd.DataFrame(bal_log_rs.cv_results_)
bal_log_rs_df.sort_values("mean_test_recall", ascending=False)[["param_model__class_weight",
                                                                  "param_model__C",
                                                                  "mean_test_accuracy", "mean_test_recall",
                                                                  "std_test_accuracy"]].head()

"""Doing this seems to reduce the accuracy but we'll see shortly that this is not an issue.

We then took the best model, found via our Grid-Search and Randomized Search with balanced weights.
"""

from sklearn.base import clone

# we refit the best accuracy model on all the training data
# so lets do that for the best other metric models
def manual_refit(input_model, X, y, gs, metric, disp_df=[]):
    output_model = clone(input_model)

    gs_df = pd.DataFrame(gs.cv_results_).sort_values("mean_test_"+metric, ascending=False)

    if disp_df:
        display(gs_df[disp_df].head())

    params = gs_df["params"].iloc[0]
    output_model = output_model.set_params(**params)
    output_model = output_model.fit(X, y)

    return output_model

rec_model = manual_refit(log_pipe, X_train, y_train, bal_log_rs, "recall")

print(pd.DataFrame(classification_report(y_test,
                                   rec_model.predict(X_test),
                                   labels=None,
                                   target_names=list(LE.classes_),
                                   sample_weight=None,
                                   digits=2,
                                   output_dict=True)).round(2))

coeffs = rec_model.named_steps['model'].coef_

"""We now check our confusion matrices for our two best models; The most accurate model, and the model with the best recall."""

### Now we check Confusion Matrix
FP_i = [0,1]
TN_i = [0,0]
TP_i = [1,1]
FN_i = [1,0]
# get the confusion matrix as a numpy array
confmat = confusion_matrix(y_true=y_test, y_pred=log_rs.predict(X_test))
# use the pretty function to make it nicer
pretty_confusion_matrix(confmat, LE.classes_,
                        "Best CV Accuracy Validation Confusion Matrix",
                        highlight_indexes=[FP_i,FN_i,TP_i,TN_i]
                       )

# get the confusion matrix as a numpy array
confmat = confusion_matrix(y_true=y_test, y_pred=rec_model.predict(X_test))
# use the pretty function to make it nicer
pretty_confusion_matrix(confmat, LE.classes_,
                        "Best CV Recall Validation Confusion Matrix",
                        highlight_indexes=[FN_i,TP_i]
                       )

"""We can see that more accurate model has less False Positives.

At first glance it would seem that the model with the higher accuracy is the better of the two, but we opted for the Best Recall model as our model of choice. As this model is of more use to the hotel owners. This is because hotels generally run on over-booking rooms because of cancellations. It's been found that hotels over-book by 15% of their capacity.

This means hotel owners would much rather assume someone will cancel and them in fact not cancelling (False-Positive case) than to assume a guest will not cancel and them cancelling (False-Negative case) leaving the owner with an empty room. We can construct our own performance metric in this case:

$$O = \frac{(FP)-(FN)}{FP+FN+TP+TN} * 100 $$

Where O is our Overbooking Percentage.

For our Best Recall Model, we have

$$O = \frac{1726-513}{1864+513+1726+4611}*100 $$

Giving us an:

$$O = 13.9\% $$

Which is a very good over-booking percentage.

The Best Accuracy model gives us:

$$O = \frac{513-1726}{1864+513+1726+4611}*100$$

Giving us an:

$$O = -8.17\%$$

A negative overbooking percentage means we actually have 8.17% of our rooms vacant at any one time, which from a hotel owner's perspective is money lost.

Now we've found our desired model, it's time to interpret what the results mean in terms of our features. We extracted the coefficients from the model and exponentiated them to arrive at 'odds' ratios. We did this as it'd be easier to explain what each coffecient means. We seperated the odd's ratios into two data frames, odd's ratios greater than 1, i.e these features increase the chances of a booking being cancelled, and a data frame with odd's ratios less than 1, i.e features that decrease the likelihood of a reservation being cancelled.
"""

X_train.head()
collist = []

for j in X_train.columns:
  collist.append(j)

beta = pd.DataFrame(columns=[collist])
betalist = []


for i in range(len(coeffs[0])):
  cb = coeffs[0][i]
  cu = np.exp(coeffs[0][i])
  betalist.append(cu)


beta = pd.DataFrame(data=betalist)
#beta = pd.DataFrame.transpose(beta)


beta
#beta.columns = [collist]
beta.head()
betamore = beta[beta>1]
betamore = pd.DataFrame.transpose(betamore)
betamore.columns = [collist]
betamore = betamore.dropna(axis=1)


betaless = beta[beta<1]
betaless = pd.DataFrame.transpose(betaless)
betaless.columns = [collist]
betaless = betaless.dropna(axis=1)

betaless.sort_values(0,axis=1,ascending=True)

countryless = betaless.filter(regex='country')
roomless =betaless.filter(regex='room_type')
mealless = betaless.filter(regex='meal')
marketless = betaless.filter(regex='market')
distless = betaless.filter(regex='distribution')
custless = betaless.filter(regex='customer')

betamore.sort_values(0,axis=1,ascending=False)
countrymore = betamore.filter(regex='country')
roommore =betamore.filter(regex='room_type')
mealmore = betamore.filter(regex='meal')
marketmore = betamore.filter(regex='market')
distmore = betamore.filter(regex='distribution')
custmore = betamore.filter(regex='customer')

betamore = betamore[betamore.columns.drop(list(betamore.filter(regex='country')))]
betamore = betamore[betamore.columns.drop(list(betamore.filter(regex='room_type')))]
betamore = betamore[betamore.columns.drop(list(betamore.filter(regex='meal')))]
betamore = betamore[betamore.columns.drop(list(betamore.filter(regex='market')))]
betamore = betamore[betamore.columns.drop(list(betamore.filter(regex='distribution')))]
betamore = betamore[betamore.columns.drop(list(betamore.filter(regex='customer')))]

betaless = betaless[betaless.columns.drop(list(betaless.filter(regex='room_type')))]
betaless = betaless[betaless.columns.drop(list(betaless.filter(regex='country')))]
betaless = betaless[betaless.columns.drop(list(betaless.filter(regex='meal')))]
betaless = betaless[betaless.columns.drop(list(betaless.filter(regex='market')))]
betaless = betaless[betaless.columns.drop(list(betaless.filter(regex='distribution')))]
betaless = betaless[betaless.columns.drop(list(betaless.filter(regex='customer')))]

betaless.head()



"""We considered other models aswell. The models we considered were Support Vector Machines (SVM) and Neural Networks.

For the Neural Network, although it returned a higher accuracy (80%) than the logistic regression, it returned less interpretable results, and we have seen that a higher accuracy can result in 'under booking'. We also couldn't alter the neural network to focus on recall rather than precsion due to Neural Networks having a more 'black-box' model.

For the SVM model, the accuracy returned was lower than the basic logistic regression before tuning, this is likely due to the large number of features we needed to feed into the model, we had 57 feature columns in our training set, and 25 unique features before encoding.

## 4. Discussion & Conclusions

*In this section you should provide a general overview of your final model, its performance, and reliability. You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.*

*This should be written with a target audience of the client who is with the hotel data and university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. Your goal should be to convince this audience that your model is both accurate and useful.*

*Keep in mind that a negative result, i.e. a model that does not work well predictively, that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explinations / justifications.*

Our final model ended up being a Logistic Regression model run for 1000 iterations, with a regularization strength (C) of 0.996303, and a balanced class weight.

It's performance measured by our own 'OverBooking' Metric, returning an 'overbooking' value of 15.7% alongside a model accuracy of 72.6%. An overbooking value of 15% can be considered realistic, as most hotels tend to overbook by 10-15%[2][3].

Overbooking when used correctly maximizes profits for the hotel.[4]

It's reproducability was quite high as seen by the 10-fold cross validation, which returned a standard deviations in accuracy of zero.

(Still to be done talk about beta coefficients)
"""



"""## References

[1] Voloshyn, P. (2019, November 19). Preprocessing: Encode and KNN impute all categorical features fast. Medium. Retrieved April 19, 2022, from https://towardsdatascience.com/preprocessing-encode-and-knn-impute-all-categorical-features-fast-b05f50b4dfaa

[2] https://hsb.shr.global/learning-center/overbooking-howtodoittherightway

[3] https://www.stayntouch.com/blog/overbooking-your-hotel/#:~:text=climb%20higher%20in%202019%20%E2%80%94%20to,the%20travel%20research%20company%20STR.

[4] Vladimir Sashov Zhechev (2010, September).
The Impact of Overbooking on Hotelsâ€™ Operation Management
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1698103

[5] https://www.nytimes.com/2019/02/18/business/hotels-overbooked-walking-travel.html#:~:text=%E2%80%9CIn%20the%20current%20climate%2C%20hotels,Dr.
"""