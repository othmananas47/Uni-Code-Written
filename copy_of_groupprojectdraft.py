# -*- coding: utf-8 -*-
"""Copy of GroupProjectDraft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xtikj4bUBIl7itWr4aTgka2D32A0uE8B

# Machine Learning in Python - Project 1

Due Friday, March 18th by 5 pm.

*Anas Othman, Fanny Coa, Matthew Cox, Shahnazam Shahid*

## 0. Setup
"""

#@title
# Add any additional libraries or submodules below

# Data libraries
import pandas as pd
import numpy as np

# Plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Plotting defaults
plt.rcParams['figure.figsize'] = (8,5)
plt.rcParams['figure.dpi'] = 80

# sklearn modules
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.metrics import mean_squared_error

#@title
from sklearn.preprocessing import PolynomialFeatures # make PolynomialFeatures
import warnings # prevent warnings
from time import time
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold
from scipy.stats.distributions import uniform, loguniform

from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from imblearn.metrics import classification_report_imbalanced
import re
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

#@title
#Functions
#model fit from Workshop 5
def model_fit2(m, X, y, plot = False):
    """Returns the root mean squared error of a fitted model based on provided X and y values.

    Args:
        m: sklearn model object
        X: model matrix to use for prediction
        y: outcome vector to use to calculating rmse and residuals
        plot: boolean value, should fit plots be shown
    """

    y_hat = m.predict(X)
    rmse = np.sqrt(mean_squared_error(y, y_hat))

    res = pd.DataFrame(
        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}
    )

    if plot:
        plt.figure(figsize=(12, 6))

        plt.subplot(121)
        sns.lineplot(x='y', y='y_hat', color="grey", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))
        sns.scatterplot(x='y', y='y_hat', data=res).set_title("Fit plot")

        plt.subplot(122)
        sns.scatterplot(x='y', y='resid', data=res).set_title("Residual plot")
        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors="black")

        plt.subplots_adjust(left=0.0)

        plt.suptitle("Model rmse = " + str(round(rmse, 4)), fontsize=16)
        plt.show()

    return rmse

#Manipulated reg_tree from Workshop 6
def reg_tree(x_train, x_test, y_train, y_test,
             n_bins=10, strategy = "uniform",plot_res = True):
    p = make_pipeline(
        KBinsDiscretizer(n_bins=n_bins, strategy=strategy, encode="onehot-dense"),
        LinearRegression(fit_intercept=False) # Since we are using onehot above we
    )                                         # need to remove the intercept here

    m = p.fit(x_train,y_train)

    pred_col = m.predict(x_test)

    rmse = np.sqrt(mean_squared_error(y_test, pred_col))

    if plot_res:
      sns.scatterplot(x = y_test, y = pred_col)

    return (rmse, pred_col)

#get_coefs
def get_coefs(m):
    """Returns the model coefficients from a Scikit-learn model object as an array,
    includes the intercept if available.
    """

    # If pipeline, use the last step as the model
    if (isinstance(m, sklearn.pipeline.Pipeline)):
        m = m.steps[-1][1]


    if m.intercept_ is None:
        return m.coef_

    return np.concatenate([[m.intercept_], m.coef_])

#model_fit
def model_fit(m, X, y, plot = False):
    """Returns the root mean squared error of a fitted model based on provided X and y values.

    Args:
        m: sklearn model object
        X: model matrix to use for prediction
        y: outcome vector to use to calculating rmse and residuals
        plot: boolean value, should fit plots be shown
    """

    y_hat = m.predict(X)
    rmse = np.sqrt(mean_squared_error(y, y_hat))

    res = pd.DataFrame(
        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}
    )

    if plot:
        plt.figure(figsize=(12, 6))

        plt.subplot(121)
        sns.lineplot(x='y', y='y_hat', color="grey", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))
        sns.scatterplot(x='y', y='y_hat', data=res).set_title("Fit plot")

        plt.subplot(122)
        sns.scatterplot(x='y', y='resid', data=res).set_title("Residual plot")
        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors="black")

        plt.subplots_adjust(left=0.0)

        plt.suptitle("Model rmse = " + str(round(rmse, 4)), fontsize=16)
        plt.show()

    return rmse

#@title
# Load data
import os
from google.colab import drive

drive.mount('/content/drive')
os.chdir('drive/My Drive/Colab Notebooks/mlp/project-1')
!ls

d = pd.read_csv("the_office.csv")

#@title
d.info()

d.head()
#print(d['episode_name'].to_string())

"""Checking to see if there is any missing values in the given dataset and as we can see there is no NaN values."""

#@title
d.isnull().sum()

"""## 1. Introduction

**------ DELETE ------**

*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client). If you use any additional data sources, you should introduce them here and discuss why they were included.*

*Briefly outline the approaches being used and the conclusions that you are able to draw.*


**------ DELETE -------**

We have been hired by NBC Universal to help advise on the creation of a special reunion episode of The Office. We are particularly interested in what makes some episodes more popular than others. We will use the data that contains every episode name, IMDb rating, total number of votes, directors, writers along with many other features.

We wish to create a predictive model that captures the underlying relationships between these features and the audience ratings, and then use the insights we gain from this model to advise NBC Universal on what they should do to produce the highest rated reunion episode possible.


We ultimately decided on a multi-linear regression model as our predictive model. Using this model, it was found that 'Number of lines', 'Number of Writers','Number of Stage directions' and the presence of the main character 'Michael' all had a positive effect on the IMDB rating of an episode. Therefore in the question of 'How can we produce a highly rated reunion episode?', NBC universal should seek to maximize these features in the production of the episode.

## 2. Exploratory Data Analysis and Feature Engineering

*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling. Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.*

*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data. Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*

*All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.*

We decide to produce a correlation heatmap and create a pairplot of all of the faetures to see how each feature interacts with each other. We can see that there is a high correlation of 0.92 between the number of lines and the number of words. This is expected as if there is more words then we can assume that there would be more lines though this is not always the case.
"""

#@title
# Creates a pair plot
sns.pairplot(d)

#@title
### Correlation Heatmap with the above features.
sns.heatmap(d.corr(), annot=True, fmt='.2f', linewidths=2)
plt.title("Correlation Heatmap")
plt.show()

"""Investigating the data for the directors of each episode we noticed there were a few typos in the names of the directors, we resolved these and any other typos that might've been present in other columns of data using the following code:

###**Typos in the data**
We will check the `director`, `writer` and `main_chars` columns for any typos. We will first check the `director` column. There are a few episodes with more than one director, which is separated with a `;`. First we will separate the names if there are more than one, using the function `.str.split(`;`,expand=True)`.
"""

#@title
### SPELLING ERRORS IN NAMES
#director columns
#separating the directors' names if there are more than one
director=d['director'].str.split(';', expand = True)
director.head()

"""As we can see above, there are now two columns. This means at most there are two directors. The value of second column will be denoted `None`, if there is only one director. We will now create two arrays, which will contain the unique names from each column, `d_0` for column 0 and `d_1`. Note that when we want to retrieve unique names from the second column, we will get `None` in the unique list, we need to remove this."""

#@title
#column 0
d_0=director[0].unique()
#d_0

#@title
#column 1
d_1=director[1].unique()
#Remove None
index=np.where(d_1==None)
d_1=np.delete(d_1,index)
d_1

"""Now we need to check which director(s) in `d_1` is in `d_0`."""

#@title
#Check if any directors in d_1 is in d_0 to remove duplicates
mask=np.isin(d_0,d_1)
#Get the index of the same ones
d_0[mask]

"""This means that all the directors in `d_1`, are in `d_0`. Hence `d_0` has all the unique director names. We will sort `d_0` alphabetically to easily see if there are spelling errors."""

#@title
np.sort(d_0)

"""There are a few spelling errors (correct director):


*   'Charles McDougal', 'Charles McDougall' (Charles McDougall)
*   'Claire Scanlon', 'Claire Scanlong' (Claire Scanlon)
*   'Greg Daneils', 'Greg Daniels' (Greg Daniels)
*   'Ken Whittingham','Ken Wittingham' (Ken Whittingham)
*   'Paul Lieberstein', 'Paul Lieerstein' (Paul Lieberstein)

All the typos are in `d_0` and we want to know how many times this typos appear in the data.
"""

#@title
from collections import Counter
#all the typos are in d_0
pd.set_option("display.max_rows", None, "display.max_columns", None)
d0=Counter(director[0]).keys() # equals to list(set(words))
d0_count=Counter(director[0]).values() # counts the elements' frequency
d0_df=pd.DataFrame([d0, d0_count],index=['Names', 'Count'])
d0_df

"""As we can see from the dataframe above, the typos appear once. Now we want to fix these typos in `d`. We don't need to do this, but it will be useful for checking. We will find the indices where these typos are and using `d.iloc[x]` to check (`x` is the index where the typo is made and can be found using `np.where(d[director]==typo)`)."""

#@title
#find which episode is directed by Charles McDougal
np.where(d['director'] == 'Charles McDougal')
#(array([22]),)
#find which episode is directed by Claire Scanlong
np.where(d['director'] == 'Claire Scanlong')
#(array([167]),)
#find which episode is directed by 'Greg Daneils'
np.where(d['director'] == 'Greg Daneils')
#(array([12]),)
#find which episode is directed by 'Ken Wittingham'
np.where(d['director'] == 'Ken Wittingham')
#(array([24]),)
#find which episode is directed by 'Paul Lieerstein`
np.where(d['director'] == 'Paul Lieerstein')
#(array([54]),)

#@title
#Fixing the typos
d['director'] = d['director'].replace(['Charles McDougal'],'Charles McDougall')
d['director'] = d['director'].replace(['Claire Scanlong'],'Claire Scanlon')
d['director'] = d['director'].replace(['Greg Daneils'],'Greg Daniels')
d['director'] = d['director'].replace(['Ken Wittingham'],'Ken Whittingham')
d['director'] = d['director'].replace(['Paul Lieerstein'],'Paul Lieberstein')

#@title
#checking if the typos are correct now
#12,22,24,54,167
d.iloc[167]

"""There are a total of 55 directors in the show, as shown in the code below."""

#@title
dn=d['director'].str.split(';', expand = True)
uniquename_d=[]
row, col = dn.shape
for i in range(row):
  for j in range(col):
    if dn.loc[i,j] not in uniquename_d:
      uniquename_d.append(dn.loc[i,j])
    else:
      pass
#Remove None
uniquename_d.remove(None)
np.sort(uniquename_d)
len(uniquename_d)

"""Now we will do the same for the `writer` column, but we will use a different and much shorter way using a loop, which will append unique names to a list `uniquename_writer`."""

#@title
#writer column
writer = d['writer'].str.split(';', expand = True)
uniquename_writer=[]
row, col = writer.shape
for i in range(row):
  for j in range(col):
    if writer.loc[i,j] not in uniquename_writer:
      uniquename_writer.append(writer.loc[i,j])
    else:
      pass
#Remove None
uniquename_writer.remove(None)
np.sort(uniquename_writer)

"""There are no typos for the `writer` column. There are a total of 40 writers."""

#@title
len(uniquename_writer)

"""We will now check typos for the `main_chars` column."""

#@title
#main character column
mc = d['main_chars'].str.split(';', expand = True)
uniquename_mc=[]
row, col = mc.shape
for i in range(row):
  for j in range(col):
    if mc.loc[i,j] not in uniquename_mc:
      uniquename_mc.append(mc.loc[i,j])
    else:
      pass
#Remove None
uniquename_mc.remove(None)
np.sort(uniquename_mc)

"""Again there is no typos for the `main_char` column and there are a total of 17 main characters."""

#@title
len(uniquename_mc)

"""Now for a machine learning approach, this list of features is too long, and not all will be relevant in creating an episode that will garner a high imdb rating. We will not explore the season or the episode name as we won't be able to control this when creating the reunion episode.

### **Exploring Episodes Data**
Number of episodes per season
"""

#@title
#Grouping the seasons and episodes together to see how many episodes are in each season
d.groupby('season')['episode'].nunique()

#@title
seasons=d['season'].unique().tolist() #storing seasons in a list
episodes=d.groupby('season')['episode'].nunique().tolist() #storing number of episodes season-wise in a list

#@title
#Bar chart of episodes per season
plt.bar(seasons,episodes)
plt.title('Season-wise episodes distribution')
plt.xlabel('Season')
plt.ylabel('Number of episodes')
plt.show()

"""From this we can see that season 5 has the most episodes at 26 whereas season 1 has the least at 6 episodes. Season 4 only has 14 episodes and this is due to missing the data for episdoes 2, 4, 6 and 8. Even if these were included we would still have the second lowest amount of episodes. This is due to production having to be cut short due to a writers strike.

### **Exploring Lines Data**
"""

#@title
#Get the number of lines in each season
n_lines_season = d.groupby('season')['n_lines'].sum()

#create a table that stores and shows the number of lines in each episode and NaN if that episode is not there
#i.e. part 1 and 2 2 episodes are stored together
f = d.pivot_table(index='season', columns='episode', values='n_lines')
f

"""Here we notice something interesting, in season 4 there are quite a a few NaN values. This is due to episode 1, 3, 5 and 7 being 2 part episodes. This is also refelected in the number of lines which are double of what would otherwise be expected. This can be seen in other episodes in various season e.g. season 3 episode 10 and season 5 episode 15."""

#@title
#Bar chart of the total number of lines said per season
plt.bar(seasons,n_lines_season)
plt.title('Season-wise number of lines distribution')
plt.ylabel('Number of lines')
plt.xlabel('Season')
plt.show()

"""Season 5 has the most number of lines followed closely by season 6. Season 1 as expected has the least number of lines as they are only of 6 episodes, followed by season 4.

### **Exploring Number of Lines Containing A Stage Direction**
"""

#@title
d_directions = d.sort_values(['season','episode','n_directions']).drop_duplicates(['season','episode','n_directions'],keep='first')

#Create a table that store number of lines contanting a stage direction per episode
ff = d_directions.pivot_table(index='season', columns='episode', values='n_directions')
ff

"""Again we notice similar NaN values as when we explored number of lines, however this is not consistent with our checking of null values which showed we had no missing data. It turns out that Season 4 episode 2 does not exist within our dataset, neither do episodes 4, 6, and 8 of season 4. The reason for this is made clear when we check the episode names, we see that season 4 episode 1 is saved in the dataset as "The Job Parts 1 and 2", which means we do not have missing episodes, and that some episodes are 40 minutes long as opposed to their standard 20 minutes."""

#@title
d_direction_t=ff.sum(axis=1) # Gets the total number of directions per season
d_direction_t.plot(kind='bar') # Plot the bar chart

"""Here we see that season 7 has the most directions followed by season 5. Seasons 1 and 4 have less directions owing to the lesser amount of episodes. While season 7 has more directions, season 5 actually has more lines (as shown previously). This means that the having a high number of directions does not translate to a high number of lines and vice versa.

### **Exploring Number of Different Characters with Spoken Lines**

Here we explore the number of episodes that have different number of characters with spoken lines.
"""

#@title
# Bar chart of number of unique n_speak_char per season
d.groupby('season')['n_speak_char'].nunique().plot(kind='bar')

"""Season 5 has the most number of unique speakers. Season 9 comes in second probably due to being the last season and getting a lot of characters back to speak again.

### **Exploring Number of Words**
"""

#@title
# Creates a table that contains the number of words spoken in each episode
fff = d.pivot_table(index='season', columns='episode', values='n_words')

d_words_t=fff.sum(axis=1) # Gets the total number of word per season
d_words_t.plot(kind='bar') # Creates a bar chart of total. number of words in each season

"""We have seen that the earlier seasons have less words spoken in them, i.e. seasons 1 and 4 having the least due to the lesser amount of episodes they have. From season 5 onwards, there are a lot more words with with seasons 7 having the most, followed by 6 and 5. There may be a connection in the number of directions and the number of words as season 7 is the most for both of those categories. This is shown in having a higher correlation than the correlation between number of stage directions and number of lines.

### **Exploring Air Date Data**
"""

#@title
# Changes air-date to datatime format
d['air_date']=pd.to_datetime(d['air_date'])

"""Here we convert the air data into date time object. After doing this we split of the date into 'year', 'month' and 'day' for to help us analyse the data.  """

#@title
d['year'] = d['air_date'].dt.year
d['month'] = d['air_date'].dt.month#jan is 1,dec is 12
d['day'] = d['air_date'].dt.weekday #mon is 0,sun is 6

"""Here we see what days the show aired on. We can see that it predominately aired on Thursdays and Tuesdays. There was a single episode that aired on a Sunday.  """

#@title
# Total number of episodes being aired in each day
d['day'].value_counts()

#@title
d[d['day']==6]

"""We can see that the single episode that aired on the Saturday was well recieved but as there was only one episode. Hence, it is not conclusive to say that we should run the reunion on Saturday."""

#@title
d.groupby('season')['month'].value_counts().tail(60)

sns.countplot(x=d['month'])

#@title
d['year'].value_counts()

sns.countplot(x=d['year'])

"""The show aired mostly in April, October and November. The show aired from 2005 to 2013, with the most episodes being released in 2009. This corresponds to part of season 5 and part of season 6 which are the seasons which have the most episodes.

### **Exploring Rating Data**
"""

#@title
# Gets the mean IMDb rating and total vaotes per season
d.groupby('season')['imdb_rating','total_votes'].mean()

"""Using the correlation heatmap we can see that the votes and the IMDb rating are negatively correlated. Looking at the mean rating and the mean number of votes we see that the rating decreases in seasons 8 and 9 with the number of votes also decreasing over the seasons.  """

#@title
# Uses the describe function to get the mean, std, min, max etc. of the show in general
d['imdb_rating'].describe()

"""According to IMDb there is 188 episodes but the dataset counts The Niagara and the The Delivery as a single episode, hence there is 186 episodes. The average IMDb rating is shown to be 8.25, with the highest rated episode scoring a 9.7 and the lowest scoring episode got a 6.7."""

#@title
# Uses the describe function to get the mean, std, min, max etc. of each season
d.groupby('season')['imdb_rating'].describe()

"""Season 8 highest ranking episode was around the median rating across all episodes.\
**Point of note**: IMDb uses a weighted average system so the rating of an episode is not the average of all ratings recieved by the voters.
"""

#@title
# Creates a plot showing the number of times an IMDb rating is achieved
sns.histplot(x=d['imdb_rating'], bins=50)

"""We can see peaks 7.8, 8.2 and 8.7. We can now check the number of episodes above the mean rating:

**Number of episodes above the mean rating:**
"""

#@title
#d[d['imdb_rating']>d['imdb_rating'].mean()]

"""Here we check the number of episode that goes above the mean rating of 8.2 and find it to be 88 episodes which corresponds to 47% of the total episodes.

**Highest and lowest rated episodes:**
"""

#@title
d[d['imdb_rating']==d['imdb_rating'].max()]

#@title
d[d['imdb_rating']==d['imdb_rating'].min()]

"""Both of the highest rated episodes were written by Greg Daniels. A point of note is that season 7 "Goodbye Michael" marks the leaving of Michael of the show and he reappears after two seasons in the Finale episode of season 9. The lowest rated episode was written by Charlie Grandy and was in season 8.

### **Exploring Votes Data**
"""

#@title
# Uses the describe function to get the mean, std, min, max etc.
d.groupby('season')['total_votes'].describe()

"""Among the seasons, season 8 has the lowest mean and maximum value."""

#@title
# Creates a plot showing the number of times the same total vote (roughly) is achieved
sns.histplot(x=d['total_votes'],bins=100)

#@title
d['total_votes'].min()

"""As we can see from the graph above there is a high number of votes concentration below 2000 with the rest being spread mostly between 2000-3000 votes. The lowest total votes is 1393."""

#@title
d.sort_values(by=['total_votes'],ascending=False).head(5)

#@title
d.sort_values(by=['total_votes'],ascending=True).head(5)

"""Since most episodes' number of votes are concentrated in the 1500-3000 range, the top 4 episodes on the list here are not just highly rated (>75th percentile of the ratings-8.6) but also the most popular episodes considering the comparatively much higher number of votes they have received. The low votes suggest that after Michael left the show, the viewers lost interest and therefore they did not watch the show/did not want to vote. The highest voted episode is the Finale and the lowest voted on episode is in season 8."""

#@title
# Create a lineplot showing the total votes per season
sns.lineplot(x=d['season'], y=d['total_votes'])

"""A decreasing trend in votes can be seen as we move towards later seasons, with a sharp dip from season 7 to 8."""

#@title
#Jointplot showing relationship between Rating and Votes and their individual distributions.
sns.jointplot(x=d['imdb_rating'], y=d['total_votes'])

"""Positive linear relation seen upto certain extent between ratings and votes. High density of votes are mostly seen around 1500 upto 3000 for ratings between 8-8.8. Votes go above 5000 only at 3 instances.

### **Exploring the Directors of the Show Data**
"""

#@title
# Gets the number of unique directors in each season
d.groupby('season')['director'].nunique()

"""Season 7 had the highest number of different directors. Apart for a single episode in season 1 all episodes were directed by a different director, while for season 7, 20 out of 24 episodes were directed by a different director."""

#@title
#considering directors who directed atleast 6 episodes (season 1 length)
#head is equal to 10 as there were 10 directors who directed 6 or more episodes
ccc=d['director'].value_counts().head(10).index.tolist()

d_dir=d[d['director'].isin(ccc)]

import plotly.express as px
# Create a boxplot with the directors and the IMDb rating
fig = px.box(d_dir,x='director', y='imdb_rating', points="all",title="Relation between Directors(who directed atleast 6 episodes) and Rating",
             hover_data=["season","episode_name"])
fig.show()

"""Greg Daniels has the highest median rating while Ken Kwapis and Paul Feig have all episode ratings above 8 - except the pilot episode - along with second highest median rating. Matt Sohn's episodes comparatively have lower ratings. David Rogers who has the lowest median rating.

### **Exploring Writers of the Show Data**
"""

#@title
# Shows the different writers and how many episodes they have written for
d['writer'].value_counts().head(5)

"""There are a few episodes with more than one writer. When the order of the writers' names are different, they are treated differently. Hence, we will standardise the orders."""

#@title
#cleaning
d['writer'].replace(['Lee Eisenberg;Gene Stupnitsky'], "Gene Stupnitsky;Lee Eisenberg", inplace=True)
d['writer'].replace(['Warren Lieberstein;Halsted Sullivan'], "Halsted Sullivan;Warren Lieberstein", inplace=True)
d['writer'].replace(['Michael Schur;Lee Eisenberg;Gene Stupnitsky'], "Lee Eisenberg;Gene Stupnitsky;Michael Schur", inplace=True)

d['writer'].value_counts().head(12)

"""Mindy Kaling, B. J. Novak and Paul Lieberstein were regular writers of the show, having written the highest number of episodes."""

#@title
d.groupby('season')['writer'].nunique()

"""Season 7 had the highest number of unique writers too."""

#@title
#considering writers who directed atleast 6 episodes (season 1 length)
#head is 12 because at least 12 writers wrote for 6 or more episodes
cccc=d['writer'].value_counts().head(12).index.tolist()

d_wri=d[d['writer'].isin(cccc)]

import plotly.express as px
# Create a boxplot with the writers and the IMDb rating
fig = px.box(d_wri,x='writer', y='imdb_rating', points="all",title="Relation between Writers(who wrote atleast 6 episodes) and Rating",
             hover_data=["season","episode_name"])

fig.show()

"""Greg Daniels has the highest median rating of 8.7, followed by Paul Lieberstein who has the median rating of 8.5. Michael Schur and Justin Spitzer have all episodes rated above 8 - except one from season 8 - while Mindy Kaling and B.J.Novak have a similar ratings distribution. Aaron Shure has the lowest median rating of 8.0.

### Exploring Character Data

Examining the data under `main_chars`, the number of characters on the show was found to be 17 , with most episodes having anywhere between 7 and 13 main characters, we at first considered character sets i.e. a set of characters that appear on the episode together, but this was found to produce to small a sample size, we then considered character pairs, but this was found to increase the complexity of this feature greatly as for 16 characters there are 120 different character pairings. We finally decided to see if having a particular character on the episode affected the 'IMDB rating in a significant way, and this was indeed found to be the case.

The following code, will split and sort the main characters while also replacing missing characters with NA values, which will create an even and uniform dataframe. This is to see if any character has a significant relationship with ratings.
"""

#@title
# Create a copy of the dataframe d
d_one = d.copy()

# Empty variable
directorlist = []
for i in d_one["director"]:
  count = 1
  if ";" in i:
    count +=1
    #print(count,i)
  directorlist.append(count)

writerlist = []
for h in d_one["writer"]:
  counts =1
  if ";" in h:
    newh = h.replace(";","space",1)
    counts += 1
    if ";" in newh:
      counts +=1
  writerlist.append(counts)
  #print(counts,h)

d_one["number_of_directors"] = directorlist
d_one["number_of_writers"]= writerlist

### We have two new featuers now.

#@title
char_mains = d_one['main_chars'].str.split(';', expand = True)

#@title
#create function to check, sort, and replace names
new_char_main = char_mains.copy()
def change_name(column, name, data, newdata):
  #note: need newdata so don't lose data with replacement
  #for every row
  for index, row in data.iterrows():
    #if the column does not contain the name
    if data.iloc[index,column] != name:
      #if any position in the row contain the name
      if any(data.iloc[index,:] == name):
        #replace that column with the name in new data set
        newdata.iloc[index,column] = name
      #if not in any position
      else:
      #NA
        newdata.iloc[index,column] = np.NaN

#@title
#for every column run change name function
names = char_mains.iloc[185,:].unique()
for i in range(17):
  change_name(i, names[i], char_mains, new_char_main)

#@title
new_char_main.head(7)

"""Now we turn the characters into dummy variables for potential use in our model, where 1 = present, and 0 = missing from an episode."""

#@title
#convert main characters to dummy variables
dummy_char = new_char_main.fillna(0)
for col in dummy_char:
  dummy_char[col] = np.where(dummy_char[col] != 0 , 1, dummy_char[col])
dummy_char.head(7)

#@title
#rename columns
dummy_df = pd.DataFrame(dummy_char)
dummy_df.columns = names
#add to dataframe and drop unrelevant columns
office_4_model = pd.concat([d_one, dummy_df], axis = 1)
office_4_model = office_4_model.drop(['episode_name', 'director', 'writer',
                                      'air_date', 'main_chars'], axis = 1)
#sum rows for new column: # of characters
office_4_model['number_characters'] = office_4_model[names].sum(axis=1).astype(int)
#check relationship
sns.pairplot(office_4_model[['Michael', 'number_characters', 'imdb_rating']])

"""It appears that the characters themselves (i.e. individually), do not contribute to any apparent relationship with the rating apart from one. To explore further we would have to check for interactions with other characters, which would add too many variables (for just two character interactions 16 choose 2 = 120, so for partial/all combinations, too many)

The one character that does seem to have an effect individually is Michael, with the most extreme being visible when looking at the ratings for his last appearance and return appearance.

In reference to number of characters, there is too much variation to discern a relationship, and there is very low linear and non-linear correlation values

Although not scarce on variables, viewer appeal does not appear clearly related to those provided. It might come down to quality of skits or character development which is hard to quantify or measure. An opinion poll on potential ideas could provide a better understanding of how to optimise rating.
"""

#@title
office_4_model.head()

"""## 3. Model Fitting and Tuning

The model we finally agreed upon was the multi linear regression model.

We first used the following 3 features,

1.   Number of lines.
2.   Number of Directions.
3.   Presence of Michael in the episode.

We split the dataset into training and test sets, with a test size of 30%.
"""

#@title
#Linear Regression
X1full=office_4_model[['n_lines', 'n_directions', 'Michael']]
y1=office_4_model['imdb_rating']

X1_train, X1_test, y1_train, y1_test = train_test_split(X1full, y1,test_size=0.3,random_state=23)
X1 = np.c_[X1_train.n_lines, X1_train.n_directions,
           X1_train.Michael]

"""Now we've got our training and test sets, we used Scikkit Learn's linear regression to fit the data training data, and we obtained or $\beta$ coefficients."""

#@title
import sklearn
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV, KFold

l = LinearRegression().fit(X = X1, y = y1_train)
beta = l.coef_
print('coefficients not including intercept')
print(beta)

"""The coefficents are in the order of the features, therefore we see that $\beta_1$ concerns number of lines, $\beta_2$:number of stage directions and
$\beta_3$ being associated with Michael's presence.

As expected $\beta_3$ is the largest, as it is a binary indicator, with 1 meaning Michael is present, and zero otherwise.

As our output variable is IMDB rating, the conclusion we can draw from these $\beta$'s is that according to our model, if we increase the number of lines by 1, we would expect to see a 0.0016 increase in the imdb rating. Which although might not look significant, the feature number of lines ranges between $10^2$ and $10^3$, and so increasing by 100 lines is not unrealaistic, and would be responsible for 1.6 increase in IMDB rating.

If we increased Number of stage directions, we expect IMDB rating to increase  by 0.00308.

And finally, if we included Michael in the episode, we expect an increase of 0.5557 in the IMDB rating.

We then used our linear regression model, to predict IMDB ratings and plotted the fit of the training data.
"""

#@title
IMDB_PRED1 = l.predict(X1)
sns.scatterplot(IMDB_PRED1, y1_train).set(title = 'Fit of training data')

plt.xlim(7,9.5)
plt.show()

"""At this point we want to know how well our model was doing before investigating further, we checked for the following metrics:

1.   Mean squared error
2.   $R^2$ score
3.   Max error



"""

#@title
from sklearn.metrics import max_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

imdbtest_pred1 = l.predict(X1_test)

print(mean_squared_error(y1_test,imdbtest_pred1))
print(r2_score(y1_test,imdbtest_pred1))
print(max_error(y1_test,imdbtest_pred1))

"""We then decided to fit the model on the testing set, and wanted to see the Model's root mean squared error (RMSE), this metric is what we used to compare models."""

#@title
X_test_matrix1 = np.c_[X1_test.n_lines, X1_test.n_directions,
                       X1_test.Michael]
model_fit2(l, X_test_matrix1, y1_test, plot=True)

"""From the 'Fit plot' above, we see that our model has a positive correlation between $y$ and $\hat{y}$ and can be used generically. However, as can be seen by the residual plot, there is a pattern to our residuals going from negative to positive as the true values of $y$ increase, which indicates we have not captured all the meaningfull features in our model.

By this point, we had investigated other models, with the same 3 features as above. Using RMSE to compare models it was found that the simple Multiple Linear regression model described above was the best model for our chosen features.

The models we considered were Regression trees, Lasso, Ridge Regression and Logistic Regression.

For the logistic regression, we transformed our output variable into 'Good' and 'Bad' episodes, where we varied the percentile for an episode to be in which would qualify it for a good episode. This model proved to be really inaccurate with low accuracy scores. We deduce that the logistic regression approach was inefficient due to the rarity of 'Good' episodes in the data set this is further amplified by the range of values IMDB took. A quick glimpse into the percentiles, if an episode had an IMDB rating of 8.2, it would be in the top 50%, which means a rating of 8.2 only qualifies an episode as above average.

We explored Lasso and Ridge regression with a range of $\alpha$ values and all proved to have lower RMSE scores than our simple Multiple Regression.

We could have predicted that Ridge regression might not have been suitable, as we had few features 3 to be exact. Ridge regression is usually used when there are more features than observations.

Lasso is also used for shrinking coefficients, and so it too would not have been suitable in our case.

The last model we considered was a regression tree model. For the regression trees we saw that a uniform apaproach was better than a quantile approach, but both had higher RMSE values than our multiple linear regression model. And for this reason we decided we will not be using this model.

After exploring other models, and realizing our multiple linear regression model was the best model, we returned to it and attempted to minimize the RMSE.

From the information we could gather from the residual plot coupled with our goal of minimizing the RMSE, we found that introducing the feature 'Number of writers' achieved this desired effect on the RMSE, and also was an attempt to remedy the fact that our 3 features used previously were not all the 'important' features of the dataset.

We ran the model exactly the same as before but with this added feature.
"""

#@title
#Linear Regression
Xfull=office_4_model[['n_lines', 'n_directions', 'number_of_writers', 'Michael']]
y=office_4_model['imdb_rating']

X_train, X_test, y_train, y_test = train_test_split(Xfull, y,test_size=0.3,random_state=23)
X = np.c_[X_train.n_lines, X_train.n_directions,
          X_train.number_of_writers, X_train.Michael]

#@title
import sklearn
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV, KFold

l = LinearRegression().fit(X = X, y = y_train)
beta = l.coef_
print('coefficients not including intercept')
print(beta)

"""As expected the $\beta$'s of the features in our initial model are now slightly smaller, this is because we have an additional feature resulting in the other coefficients' 'importance' decreasing."""

#@title
from sklearn.metrics import max_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

#@title
print(mean_squared_error(y_test,imdbtest_pred))
print(r2_score(y_test,imdbtest_pred))
print(max_error(y_test,imdbtest_pred))

"""We welcomed this reduction in Max error and Mean squared error, indicating to us that the addition of 'Number of Writers' had improved the model. The $R^2$ score had increased, but this is expected if we added another feature, it will increase the variation described the model. We soon discuss why $R^2$ is not such an important metric in our particular case"""

#@title
X_test_matrix = np.c_[X_test.n_lines, X_test.n_directions,
                      X_test.number_of_writers, X_test.Michael]
model_fit2(l, X_test_matrix, y_test, plot=True)

"""We see that the addition of the feature 'Number of Writers' resulted in a lower RMSE value which is was we set out to achieve with the tuning of the model. We also were pleased to see we had managed to reduce additional metrics in the model. Concerning the Fit plot, we see we still have the positive association of $y$ and $\hat{y}$. The residual plot, still showing a pattern between $y$ and the residuals. We note here that a $y$ value of 8.2 is the considered the cutoff for the 50th percentile, we can see this from the following code:"""

#@title
from numpy import percentile

p = percentile(y,50)
print(p)

"""And so using this information with our residual plot, we see that we have residuals around the mean, and higher on the outliers (Very good Episodes, Very bad episodes). This behaviour is expected due to the scarcity of these episodes. We can see this from the percentile code below, for an episode to go up 5% from 90% to 95% it would have to increase 0.375 in IMDB rating"""

#@title
p1 = percentile(y,90)
print(p1)

p2 = percentile(y,95)
print(p2)

"""But for an episode to go from the 85th percentile to the 90th. It only needs to increase by 0.1 in IMDB rating."""

#@title
p1 = percentile(y,85)
print(p1)

p2 = percentile(y,90)
print(p2)

"""This uneven distribution of the ratings of the episodes made it hard to judge the accuracy of the model as we have no problem predicting what it takes to be an 'average' episode, but have trouble predicting what the properties of a 'Very good' episode and a 'Very bad' episode.

## 4. Discussion & Conclusions

The model we decided to go with in the end was a multiple linear regression model with our chosen features. We fit a baseline model choosing to focus on the 4 features:

1.   Number of lines
2.   Number of Stage Directions
3.   Number of writers
4.   Presence of Michael in the episode


The regression model returned positive $\beta$ coefficients for each of these features, meaning if we increased these features the IMDB_rating of the episode is likely to be high, and ofcourse the presence of Michael in the episode being represented as an indicator, either 0 or 1, we seen that the change associated in Michael being in the episode was found to be a positive 0.55 change in the IMDB rating, The highest $\beta$ coefficient recorded.

The $\beta$ coefficients were found to be:


```markdown
```

β coefficent | Feature
-------------------|------------------
0.00159951      | Number of Lines
0.00322315      | Number of Stage Directions
0.08840776      | Number of Writers
0.54631335       | Michael being Present

---

Concerning the reliability, we evaluated the model's RMSE and found it to be
0.4545, the table below shows other scoring metrics we thought would be useful

```markdown
```

Score | Scoring metric
-------------------|------------------
0.2066      | Mean Squared Error
0.3773      | $R^2$ Score
1.02952      | Max Error


---

The mean squared error is average of the squares of the error, and we want this to be as low as possible, preferably zero. A score of 0.2066 for our rating method being out of 10.00 means the average error in predicitng IMDB ratings in our model is bounded by 2%.

$R^2$ Score is the proportion of the variance in the dependent variable that is predictable from the independent variable. This scoring metric ranges from 0 to 1, with 1 being perfectly correlated and 0; no correleation at all. Although 0.3773 might not seem like a good score for our model, it's important to note that $R^2$ does not measure 'goodness' of fit it is however is a good measure of how 'noisy' our data is.

Max Error is the worst case error between our predicted values given by the model and the true values. This is a very important metric in our case, as it means that if we follow the guidance given by the model,in the worst case scenario, the IMDB_rating of the reunion episode will be 1.03 less than what we would have hoped for.

The highest rated episode in our dataset had an IMDB rating of 9.7 meanwhile the lowest had a rating of just 6.7, in the worst case scenario following hte model where we'd assume we use the $\beta$ coefficents that result in the highest output,
"""